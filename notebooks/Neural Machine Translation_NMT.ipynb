{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHBP5xzmrM4U"
   },
   "source": [
    "# Neural Machine Translation (NMT) -English to French\n",
    "\n",
    "\n",
    "\n",
    "![NMT system](https://miro.medium.com/max/1928/1*CkeGXClZ5Xs0MhBc7xFqSA.png)\n",
    "\n",
    "\n",
    "For better understood how seq2seq work your can look\n",
    "\n",
    "[Andrew Ng course ](https://www.youtube.com/playlist?list=PL1F3ABbhcqa3BBWo170U4Ev2wfsF7FN8l) on youtube \n",
    "\n",
    "Keras Tutorial Machine Translation [lstm seq2seq](https://keras.io/examples/nlp/lstm_seq2seq/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qK2TWV1nm48Q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import  Adam , RMSprop\n",
    "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
    "import pandas as pd\n",
    "from my_utils import load_data , split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27OzmS-MIymc"
   },
   "outputs": [],
   "source": [
    "\n",
    "!wget http://www.manythings.org/anki/fra-eng.zip -O fra-eng.zip\n",
    "!unzip fra-eng.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGmBbVVTrm74"
   },
   "outputs": [],
   "source": [
    "data = load_data(\"fra.txt\")\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZvqOrEO6jzo"
   },
   "outputs": [],
   "source": [
    "eng_corpus , french_corpus =  split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2_ux1rZnDyY"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(eng_corpus) \n",
    "eng_word_index = tokenizer.word_index\n",
    "num_eng_tokens = len( eng_word_index )+1\n",
    "eng_sequences= tokenizer.texts_to_sequences(eng_corpus) \n",
    "max_input_length_sequences =  max([len(x) for x in  eng_sequences]) \n",
    "\n",
    "\n",
    "padded_eng_sequences = pad_sequences(eng_sequences , maxlen=max_input_length_sequences , padding='post' )\n",
    "encoder_input_data = np.array(padded_eng_sequences )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deB0oX_0pj8R"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts( french_corpus) \n",
    "\n",
    "french_word_index = tokenizer.word_index\n",
    "num_french_tokens = len( french_word_index )+1\n",
    "\n",
    "french_sequences = tokenizer.texts_to_sequences( french_corpus) \n",
    "max_output_length_sequences =  max([len(x) for x in  french_sequences]) \n",
    "\n",
    "\n",
    "\n",
    "padded_french_sequences = preprocessing.sequence.pad_sequences( french_sequences , maxlen=max_output_length_sequences, padding='post' )\n",
    "decoder_input_data = np.array( padded_french_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPCTmeL7qj3T"
   },
   "outputs": [],
   "source": [
    "decoder_target_data = []\n",
    "for token_seq in french_sequences:\n",
    "    decoder_target_data.append(token_seq[ 1 : ]) \n",
    "    \n",
    "padded_french_lines = preprocessing.sequence.pad_sequences( decoder_target_data , maxlen=max_output_length_sequences, padding='post' )\n",
    "onehot_french_lines = utils.to_categorical( padded_french_lines , num_french_tokens )\n",
    "decoder_target_data = np.array( onehot_french_lines )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_N71uykUPbe"
   },
   "source": [
    "> Defining the Encoder-Decoder model\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "Hqb4Bps1s_Lr"
   },
   "outputs": [],
   "source": [
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(max_input_length_sequences,))\n",
    "encoder_embedding = tf.keras.layers.Embedding( num_eng_tokens, 256 , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 256 , return_state=True , dropout=0.2 )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(max_output_length_sequences,))\n",
    "decoder_embedding = tf.keras.layers.Embedding( num_french_tokens, 256 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 256 , return_state=True , return_sequences=True , dropout=0.2)\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( num_french_tokens , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(loss='categorical_crossentropy'  , optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9g_8sR7WWf3"
   },
   "source": [
    "### 2) Training the model\n",
    "We train the model for a number of epochs with RMSprop optimizer and categorical crossentropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "fnd2H27qt4Hy"
   },
   "outputs": [],
   "source": [
    "num_epoch=32\n",
    "batch_size= 250\n",
    "model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=num_epoch ) \n",
    "model.save( 'model.h5' ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNhVkiZLvdTq"
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 256,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 256 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_hrJcNP-mXb"
   },
   "outputs": [],
   "source": [
    "\n",
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = []\n",
    "    for word in words:\n",
    "        tokens_list.append( eng_word_index[ word ] ) \n",
    "        \n",
    "    return pad_sequences( [tokens_list] , maxlen=max_input_length_sequences , padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "id": "2Mfco9WKukhS"
   },
   "outputs": [],
   "source": [
    "\n",
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "enc_model.save( 'enc_model.h5' ) \n",
    "dec_model.save( 'dec_model.h5' ) \n",
    "model.save( 'model.h5' ) \n",
    "\n",
    "for epoch in range( encoder_input_data.shape[0] ):\n",
    "    states_values = enc_model.predict( str_to_tokens( input( 'Enter eng sentence : ' ) ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = french_word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in french_word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length_sequences:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qog5t4YA7qbP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Neural_Machine_Translation_( Eng-French ).ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}